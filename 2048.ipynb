{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import time\n",
    "import numpy as np\n",
    "import sys\n",
    "from tkinter import *\n",
    "import pylab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Env():\n",
    "    LEFT = 0\n",
    "    UP = 1\n",
    "    RIGHT = 2\n",
    "    DOWN = 3\n",
    "\n",
    "    ACTION_STRING = {LEFT : 'left', UP:'up' , RIGHT:'right', DOWN:'down'}\n",
    "\n",
    "    def __init__(self, width =4, height =4):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        \n",
    "        self.board = None\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.board = np.zeros((self.width,self.height),dtype = np.int64)\n",
    "        self.place_random_tiles(self.board,cnt=2)\n",
    "        return self.board\n",
    "        \n",
    "    def step(self,action:int):\n",
    "        rotated_obs = np.rot90(self.board, k=action)\n",
    "        reward, updated_obs = self.slide_left_and_merge(rotated_obs)\n",
    "        self.board = np.rot90(updated_obs, k=4 - action)\n",
    "        \n",
    "        self.place_random_tiles(self.board, cnt=1)\n",
    "        done = self.is_done()\n",
    "        \n",
    "        return self.board, reward,done,{}\n",
    "    \n",
    "    def is_done(self):\n",
    "        temp = self.board.copy()\n",
    "        if not temp.all():\n",
    "            return False\n",
    "        \n",
    "        for action in range(4):\n",
    "            rotated_obs = np.rot90(temp,k=action)\n",
    "            _,updated_obs = self.slide_left_and_merge(rotated_obs)\n",
    "            if not updated_obs.all():\n",
    "                return False\n",
    "        \n",
    "        return True\n",
    "            \n",
    "            \n",
    "        \n",
    "    def sample_tiles(self, cnt=1):\n",
    "\n",
    "        choices = [2, 4]\n",
    "        probs = [0.9, 0.1]\n",
    "\n",
    "        tiles = np.random.choice(choices,size=cnt, p=probs)\n",
    "        return tiles.tolist()\n",
    "\n",
    "    def place_random_tiles(self,board,cnt =1):\n",
    "        if not board.all():\n",
    "            tiles = self.sample_tiles(cnt)\n",
    "            tile_locations = self.sample_tile_locations(board, cnt)\n",
    "            board[tile_locations] = tiles\n",
    "    \n",
    "    def sample_tile_locations(self, board, cnt=1):\n",
    "        zero_locations = np.argwhere(board==0)\n",
    "        zero_index = np.random.choice(len(zero_locations) , size = cnt)\n",
    "        \n",
    "        zero_position = zero_locations[zero_index]\n",
    "        zero_position = list(zip(*zero_position))\n",
    "        \n",
    "        return zero_position\n",
    "    \n",
    "    \n",
    "    def slide_left_and_merge(self, board):\n",
    "        \n",
    "        result=[]\n",
    "        score = 0\n",
    "        for row in board:\n",
    "            row = np.extract(row>0,row)\n",
    "            score_, result_row = self.try_merge(row)\n",
    "            \n",
    "            score+= score_\n",
    "            row = np.pad(np.array(result_row), (0, self.width - len(result_row)), 'constant', constant_values=(0,))\n",
    "            result.append(row)\n",
    "            \n",
    "        return score, np.array(result, dtype=np.int64)\n",
    "    \n",
    "    @staticmethod\n",
    "    def try_merge(row):\n",
    "        score = 0\n",
    "        result_row = []\n",
    "        i=1\n",
    "        while i <len(row):\n",
    "            if row[i] == row[i-1]:\n",
    "                score += row[i]*2\n",
    "                result_row.append(row[i]*2)\n",
    "                i+=2\n",
    "            else:\n",
    "                result_row.append(row[i-1])\n",
    "                i+=1\n",
    "        if i==len(row):\n",
    "            result_row.append(row[i-1])\n",
    "        return score, result_row\n",
    "    \n",
    "    def render(self):\n",
    "        return self.board\n",
    "#         for row in self.board.tolist():\n",
    "#             print(' \\t'.join(map(str, row)))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self,state_size,action_size):\n",
    "        self.load_model = False\n",
    "        self.render = True\n",
    "        self.discount_factor = 0.99\n",
    "        self.learning_rate = 0.00001\n",
    "        \n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay=0.99\n",
    "        self.epsilon_min = 0.01\n",
    "        \n",
    "        self.batch_size = 128\n",
    "        self.train_start = 1000\n",
    "\n",
    "        self.queueLenMax = 5000\n",
    "        self.memory = deque(maxlen = self.queueLenMax)\n",
    "        \n",
    "        self.perfWindowSize =10\n",
    "        self.penalty = -100\n",
    "        \n",
    "        \n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        self.model ,self.model_optim, self.model_loss= self.build_model()\n",
    "        self.target_model,self.target_model_optim,self.target_model_loss = self.build_model()\n",
    "    \n",
    "        self.update_target_model() # set same weights at the first time\n",
    "        \n",
    "        if self.load_model:\n",
    "            self.model.load_state_dict(torch.load(\"./2048_dqn_trained.pth\"))\n",
    "            self.epsilon_decay =1 \n",
    "            self.epsilon = 0\n",
    "            self.epsilon_min = 0\n",
    "\n",
    "        \n",
    "    def build_model(self):\n",
    "        \n",
    "        model = nn.Sequential(nn.Conv2d(1,16, kernel_size=2,stride=1),\n",
    "                              nn.ReLU(),\n",
    "                              nn.Conv2d(16,32, kernel_size=2,stride=1),\n",
    "                              nn.ReLU(),\n",
    "                              nn.Conv2d(32,64, kernel_size=2,stride=1),\n",
    "                              nn.ReLU(),\n",
    "                              nn.Flatten(),\n",
    "                              nn.Linear(64,128),\n",
    "                              nn.ReLU(),\n",
    "                              nn.Linear(128,16),\n",
    "                              nn.ReLU(),\n",
    "                              nn.Linear(16,4))\n",
    "        model.to('cuda')\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr = self.learning_rate)\n",
    "        loss = nn.MSELoss()\n",
    "\n",
    "        return model,optimizer,loss\n",
    "    \n",
    "    def update_target_model(self):\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "        \n",
    "    \n",
    "    def get_action(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            state =torch.tensor([state],dtype =torch.float32).reshape(-1,1,4,4).to('cuda')\n",
    "            q_value = self.model(state)\n",
    "            q_value=q_value.cpu().detach().numpy()\n",
    "            return np.argmax(q_value[0])\n",
    "        \n",
    "    def append_sample(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "            \n",
    "    def train_model(self):\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "        else:\n",
    "            self.epsilon = self.epsilon_min\n",
    "            \n",
    "        mini_batch = random.sample(self.memory, self.batch_size)\n",
    "\n",
    "        states = np.zeros((self.batch_size, self.state_size))\n",
    "        next_states = np.zeros((self.batch_size, self.state_size))\n",
    "        actions, rewards, dones = [], [], []\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            states[i] = mini_batch[i][0]\n",
    "            actions.append(mini_batch[i][1])\n",
    "            rewards.append(mini_batch[i][2])\n",
    "            next_states[i] = mini_batch[i][3]\n",
    "            dones.append(mini_batch[i][4])\n",
    "    \n",
    "        \n",
    "        states = torch.tensor(states, dtype =torch.float32).reshape(-1,1,4,4).to('cuda')\n",
    "        next_states = torch.tensor(next_states,dtype =torch.float32).reshape(-1,1,4,4).to('cuda')\n",
    "        \n",
    "        target = self.model(states)\n",
    "        target_val = self.target_model(next_states)\n",
    "    \n",
    "        target_val = target_val.cpu().detach().numpy()\n",
    "        \n",
    "        for i in range(self.batch_size):\n",
    "           \n",
    "            if dones[i]:\n",
    "                target[i][actions[i]] = rewards[i]\n",
    "            else:\n",
    "                target[i][actions[i]] = rewards[i] + self.discount_factor * (np.amax(target_val[i]))\n",
    "\n",
    "        # and do the model fit!\n",
    "        self.model.train()\n",
    "        self.model_optim.zero_grad()\n",
    "        \n",
    "        output = self.model(states)\n",
    "        loss = self.model_loss(output,target)\n",
    "        loss.backward()\n",
    "        self.model_optim.step()\n",
    "        \n",
    "      \n",
    "            \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Moves: 123 Score 1108\n"
     ]
    }
   ],
   "source": [
    "if __name__ =='__main__':\n",
    "\n",
    "    env = Env()\n",
    "    env.reset()\n",
    "    env.render()\n",
    "    score = 0\n",
    "    done = False\n",
    "    moves = 0\n",
    "    while not done:\n",
    "        action = np.random.choice(range(4), 1).item()\n",
    "#         action = 0\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        moves += 1\n",
    "        score+=reward\n",
    "\n",
    "#         print('Next Action: \"{}\"\\n\\nReward: {}'.format( env.ACTION_STRING[action], reward))\n",
    "#         print(env.render())\n",
    "    #     time.sleep(0.1)\n",
    "\n",
    "    print('Total Moves: {} Score {}'.format(moves,score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0   score: 1480  move: 159   memory length: 159   epsilon: 1.0\n",
      "episode: 1   score: 2828  move: 248   memory length: 407   epsilon: 1.0\n",
      "episode: 2   score: 988  move: 111   memory length: 518   epsilon: 1.0\n",
      "episode: 3   score: 424  move: 78   memory length: 596   epsilon: 1.0\n",
      "episode: 4   score: 580  move: 78   memory length: 674   epsilon: 1.0\n",
      "episode: 5   score: 388  move: 65   memory length: 739   epsilon: 1.0\n",
      "episode: 6   score: 1668  move: 178   memory length: 917   epsilon: 1.0\n",
      "episode: 7   score: 1288  move: 133   memory length: 1050   epsilon: 0.5989560064661611\n",
      "episode: 8   score: 220  move: 45   memory length: 1095   epsilon: 0.38104711810454983\n",
      "episode: 9   score: 396  move: 67   memory length: 1162   epsilon: 0.19432859888279505\n",
      "episode: 10   score: 1100  move: 200   memory length: 1362   epsilon: 0.026036082493920133\n",
      "episode: 11   score: 1384  move: 2908   memory length: 4270   epsilon: 0.01\n",
      "episode: 12   score: 220  move: 457   memory length: 4727   epsilon: 0.01\n",
      "episode: 13   score: 260  move: 160   memory length: 4887   epsilon: 0.01\n",
      "episode: 14   score: 268  move: 274   memory length: 5000   epsilon: 0.01\n",
      "episode: 15   score: 816  move: 544   memory length: 5000   epsilon: 0.01\n",
      "episode: 16   score: 956  move: 1107   memory length: 5000   epsilon: 0.01\n",
      "episode: 17   score: 856  move: 2120   memory length: 5000   epsilon: 0.01\n",
      "episode: 18   score: 1352  move: 855   memory length: 5000   epsilon: 0.01\n",
      "episode: 19   score: 440  move: 859   memory length: 5000   epsilon: 0.01\n",
      "episode: 20   score: 1288  move: 1289   memory length: 5000   epsilon: 0.01\n",
      "episode: 21   score: 716  move: 701   memory length: 5000   epsilon: 0.01\n",
      "episode: 22   score: 688  move: 633   memory length: 5000   epsilon: 0.01\n",
      "episode: 23   score: 888  move: 844   memory length: 5000   epsilon: 0.01\n",
      "episode: 24   score: 320  move: 131   memory length: 5000   epsilon: 0.01\n",
      "episode: 25   score: 1240  move: 2473   memory length: 5000   epsilon: 0.01\n",
      "episode: 26   score: 624  move: 641   memory length: 5000   epsilon: 0.01\n",
      "episode: 27   score: 1120  move: 2085   memory length: 5000   epsilon: 0.01\n",
      "episode: 28   score: 1424  move: 2017   memory length: 5000   epsilon: 0.01\n",
      "episode: 29   score: 448  move: 405   memory length: 5000   epsilon: 0.01\n",
      "episode: 30   score: 340  move: 207   memory length: 5000   epsilon: 0.01\n",
      "episode: 31   score: 1448  move: 830   memory length: 5000   epsilon: 0.01\n",
      "episode: 32   score: 1456  move: 735   memory length: 5000   epsilon: 0.01\n",
      "episode: 33   score: 788  move: 676   memory length: 5000   epsilon: 0.01\n",
      "episode: 34   score: 404  move: 445   memory length: 5000   epsilon: 0.01\n",
      "episode: 35   score: 236  move: 267   memory length: 5000   epsilon: 0.01\n",
      "episode: 36   score: 684  move: 1202   memory length: 5000   epsilon: 0.01\n",
      "episode: 37   score: 1072  move: 1752   memory length: 5000   epsilon: 0.01\n",
      "episode: 38   score: 684  move: 1784   memory length: 5000   epsilon: 0.01\n",
      "episode: 39   score: 204  move: 133   memory length: 5000   epsilon: 0.01\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    env = Env()\n",
    "    state_size = 16\n",
    "    action_size = 4\n",
    "    \n",
    "    agent = DQNAgent(state_size, action_size)\n",
    "    \n",
    "    scores, episodes = [], []\n",
    "\n",
    "    EPISODES=1000\n",
    "    for e in range(EPISODES):\n",
    "        done=False\n",
    "        score = 0\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "        \n",
    "        move =0\n",
    "        while not done:\n",
    "#             if agent.render:\n",
    "#                 env.render()\n",
    "            move+=1\n",
    "            # get action for the current state and go one step in environment\n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            next_state = np.reshape(next_state, [1, state_size])\n",
    "            # if an action make the episode end, then gives penalty of -100\n",
    "#             reward = reward if not done or score == 499 else agent.penalty\n",
    "\n",
    "            # save the sample <s, a, r, s'> to the replay memory\n",
    "            agent.append_sample(state, action, reward, next_state, done)\n",
    "            # every time step do the training\n",
    "            if agent.load_model is False and (len(agent.memory) >= agent.train_start):\n",
    "                agent.train_model()\n",
    "                \n",
    "            score += reward\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                # every episode update the target model to be same with model\n",
    "                if agent.load_model is False:\n",
    "                    agent.update_target_model()\n",
    "\n",
    "                # every episode, plot the play time\n",
    "#                 score = score if score == 500 else score + 100\n",
    "                scores.append(score)\n",
    "                episodes.append(e)\n",
    "                pylab.plot(episodes, scores, 'b')\n",
    "                pylab.title('Score per episode')\n",
    "                pylab.savefig(\"./2048_dqn.png\")\n",
    "                print(\"episode:\", e, \"  score:\", score, \" move:\", move,\"  memory length:\", len(agent.memory), \"  epsilon:\", agent.epsilon)\n",
    "\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2048 with Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2048\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random as rand\n",
    "import time\n",
    "from tkinter import *\n",
    "import tkinter as tk\n",
    "\n",
    "FONT = (\"Verdana\", 40, \"bold\")\n",
    "\n",
    "\n",
    "class Env(tk.Tk):\n",
    "    def __init__(self):\n",
    "        super(Env, self).__init__()\n",
    "        self.title('2048')\n",
    "        self.geometry('{0}x{1}'.format(700, 800))\n",
    "        self.SIZE = 500\n",
    "        self.GRID_LEN = 4\n",
    "        self.GRID_PADDING = 10\n",
    "        self.BACKGROUND_COLOR_GAME = \"#92877d\"\n",
    "        self.BACKGROUND_COLOR_CELL_EMPTY = \"#9e948a\"\n",
    "        self.BACKGROUND_COLOR_DICT = {2: \"#eee4da\", 4: \"#ede0c8\", 8: \"#f2b179\", 16: \"#f59563\",\n",
    "                                      32: \"#f67c5f\", 64: \"#f65e3b\", 128: \"#edcf72\", 256: \"#edcc61\",\n",
    "                                      512: \"#edc850\", 1024: \"#edc53f\", 2048: \"#edc22e\"}\n",
    "\n",
    "        self.CELL_COLOR_DICT = {2: \"#776e65\", 4: \"#776e65\", 8: \"#f9f6f2\", 16: \"#f9f6f2\",\n",
    "                                32: \"#f9f6f2\", 64: \"#f9f6f2\", 128: \"#f9f6f2\", 256: \"#f9f6f2\",\n",
    "                                512: \"#f9f6f2\", 1024: \"#f9f6f2\", 2048: \"#f9f6f2\"}\n",
    "        self.grid_cells = []\n",
    "        self.init_grid()\n",
    "        self.init_matrix()\n",
    "        self.action_space = ['u', 'd', 'l', 'r']\n",
    "        self.action_size = len(self.action_space)\n",
    "        self.counter = 0\n",
    "        self.wait_visibility()\n",
    "\n",
    "    def init_grid(self):\n",
    "        self.background = tk.Frame(self, bg=self.BACKGROUND_COLOR_GAME, width=self.SIZE, height=self.SIZE)\n",
    "        self.background.grid()\n",
    "        for i in range(self.GRID_LEN):\n",
    "            self.grid_row = []\n",
    "            for j in range(self.GRID_LEN):\n",
    "                self.cell = tk.Frame(self.background, bg=self.BACKGROUND_COLOR_CELL_EMPTY,\n",
    "                                     width=self.SIZE / self.GRID_LEN, height=self.SIZE / self.GRID_LEN)\n",
    "                self.cell.grid(row=i, column=j, padx=self.GRID_PADDING, pady=self.GRID_PADDING)\n",
    "                # font = Font(size=FONT_SIZE, family=FONT_FAMILY, weight=FONT_WEIGHT)\n",
    "                t = Label(master=self.cell, text=\"\", bg=self.BACKGROUND_COLOR_CELL_EMPTY, justify=CENTER, font=FONT,\n",
    "                          width=4, height=2)\n",
    "                t.grid()\n",
    "                self.grid_row.append(t)\n",
    "\n",
    "            self.grid_cells.append(self.grid_row)\n",
    "        self.grid_row = []\n",
    "        self.cell = tk.Frame(self.background, bg=self.BACKGROUND_COLOR_CELL_EMPTY, width=self.SIZE,\n",
    "                             height=self.SIZE / self.GRID_LEN)\n",
    "        self.cell.grid(row=4, columnspan=4, padx=self.GRID_PADDING, pady=self.GRID_PADDING, sticky=W + E + N + S)\n",
    "        t = Label(master=self.cell, text=\"\", bg=self.BACKGROUND_COLOR_CELL_EMPTY, justify=CENTER, font=FONT, width=4,\n",
    "                  height=2)\n",
    "        t.grid(columnspan=4, ipadx=250)\n",
    "        self.grid_row.append(t)\n",
    "        self.grid_cells.append(self.grid_row)\n",
    "\n",
    "    def init_matrix(self):\n",
    "        self.game = Game(self.GRID_LEN)\n",
    "        self.state = self.game.start()\n",
    "\n",
    "    def render(self):\n",
    "        time.sleep(0.01)\n",
    "        self.update()\n",
    "\n",
    "    def make_move(self, move=None):\n",
    "        #         self.title.configure(f'2048 : {self.game.score}')\n",
    "        #         print(self.game.score)\n",
    "\n",
    "        #         output = 0 # 이 부분이 인공 신경망에서 전달해준 값을 바탕으로 선택된 action에 전달될 움직임 방향 -> 나중에 적용할 때 매개변수로 받고 환경에서 부르면 될듯\n",
    "        #         move = rand.choice([0,1,2,3]) # np.argmax(output[0]) 이런식으로 신경망이 뱉어준 움직임을 선택하거나 따로 인덱스를 주거나 해서 선택하면 될듯\n",
    "        self.game.move(move)\n",
    "        self.state = self.game.getBord()\n",
    "        return self.state\n",
    "\n",
    "    def reset(self):\n",
    "        self.game.reset()\n",
    "        self.init_matrix()\n",
    "        return self.game.getBord().reshape(-1, 1)\n",
    "\n",
    "    def get_state(self):\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        self.counter += 1\n",
    "        next_state = self.make_move(action).reshape(-1, 1)\n",
    "        reward = self.game.getReward()\n",
    "        done = self.game.isDone()\n",
    "        for i in range(self.GRID_LEN):\n",
    "            for j in range(self.GRID_LEN):\n",
    "                new_number = self.state[i][j]\n",
    "                if new_number == 0:\n",
    "                    self.grid_cells[i][j].configure(text=\"\", bg=self.BACKGROUND_COLOR_CELL_EMPTY)\n",
    "                else:\n",
    "                    self.grid_cells[i][j].configure(text=str(new_number), bg=self.BACKGROUND_COLOR_DICT[new_number],\n",
    "                                                    fg=self.CELL_COLOR_DICT[new_number])\n",
    "        self.grid_cells[4][0].configure(text=\"Game Score : {}\".format(self.game.getScore()),\n",
    "                                        bg=self.BACKGROUND_COLOR_CELL_EMPTY, justify=CENTER)\n",
    "        if done:\n",
    "            self.grid_cells[1][1].configure(text=\"You\", bg=self.BACKGROUND_COLOR_CELL_EMPTY)\n",
    "            self.grid_cells[1][2].configure(text=\"Lose!\", bg=self.BACKGROUND_COLOR_CELL_EMPTY)\n",
    "            reward = self.game.getScore()\n",
    "        return next_state, reward, done\n",
    "    \n",
    "    def close(self):\n",
    "        self.destroy()\n",
    "\n",
    "\n",
    "\n",
    "class Game:\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "        self.bord = np.zeros((self.size, self.size), dtype='int64')\n",
    "        self.score = 0\n",
    "        self.reward = 0\n",
    "        self.done = False\n",
    "\n",
    "    def findEmpty(self):\n",
    "        # 빈칸을 확인해서 좌표 return / 없으면 None return\n",
    "        z_list = np.array([[r, c] for r, c in zip(np.where(self.bord == 0)[0], np.where(self.bord == 0)[1])])\n",
    "        if z_list.size == 0:\n",
    "            return None\n",
    "        location = rand.choice(z_list)\n",
    "        return location\n",
    "\n",
    "    def isfull(self):\n",
    "        # bord가 숫자로 가득 차있는경우 True/ 빈칸이 있는경우 False 반환\n",
    "        if 0 in self.bord.reshape(-1):\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "    def mkblock(self):\n",
    "        # 빈칸에 확률에 맞춰서 2또는 4 블럭을 생성\n",
    "        location = self.findEmpty()\n",
    "        self.bord[location[0]][location[1]] = 2 if rand.random() < 0.9 else 4\n",
    "\n",
    "    def merge(self, lstin):\n",
    "        # 합치는 연산\n",
    "        lst = lstin\n",
    "        if lst.size != len(set(lst)):\n",
    "            for t in range(lst.size - 1):\n",
    "                if lst[t] == lst[t + 1]:\n",
    "                    self.reward += lst[t]\n",
    "                    lst[t] *= 2\n",
    "                    lst[t + 1] = 0\n",
    "            lst = lst[np.where(lst != 0)]\n",
    "        return lst\n",
    "\n",
    "    def move(self, index):\n",
    "        self.reward = 0  # 움직이기 전에 보상을 초기화\n",
    "        ismove = False  # 움직임 후 변화가 없는 경우에는 안 움직인 것으로 판단하기위해 설정\n",
    "        # index에 따라 움직임\n",
    "        # 움직임이 가능한지 확인하고\n",
    "        #         print( self.canMove())\n",
    "        if self.canMove():\n",
    "            # 가능하다면 움직임 (0 - 상) (1 - 하) (2 - 좌) (3 - 우)\n",
    "            if index == 0:\n",
    "                for c in range(self.size):\n",
    "                    col = self.bord[:, c][np.where(self.bord[:, c] != 0)]\n",
    "                    col = self.merge(col)\n",
    "                    pd = 4 - col.size\n",
    "                    if np.any(self.bord[:, c] != np.pad(col, (0, pd))):\n",
    "                        self.bord[:, c] = np.pad(col, (0, pd))\n",
    "                        ismove = True\n",
    "\n",
    "            elif index == 1:\n",
    "                for c in range(self.size):\n",
    "                    col = self.bord[:, c][np.where(self.bord[:, c] != 0)]\n",
    "                    col = self.merge(col[::-1])[::-1]\n",
    "                    pd = 4 - col.size\n",
    "                    if np.any(self.bord[:, c] != np.pad(col, (pd, 0))):\n",
    "                        self.bord[:, c] = np.pad(col, (pd, 0))\n",
    "                        ismove = True\n",
    "\n",
    "            elif index == 2:\n",
    "                for r in range(self.size):\n",
    "                    row = self.bord[r, :][np.where(self.bord[r, :] != 0)]\n",
    "                    row = self.merge(row)\n",
    "                    pd = 4 - row.size\n",
    "                    if np.any(self.bord[r, :] != np.pad(row, (0, pd))):\n",
    "                        self.bord[r, :] = np.pad(row, (0, pd))\n",
    "                        ismove = True\n",
    "\n",
    "            elif index == 3:\n",
    "                for r in range(self.size):\n",
    "                    row = self.bord[r, :][np.where(self.bord[r, :] != 0)]\n",
    "                    row = self.merge(row[::-1])[::-1]\n",
    "                    pd = 4 - row.size\n",
    "                    if np.any(self.bord[r, :] != np.pad(row, (pd, 0))):\n",
    "                        self.bord[r, :] = np.pad(row, (pd, 0))\n",
    "                        ismove = True\n",
    "\n",
    "            if ismove:  # 움직임이 있었던 경우에만 블럭 생성\n",
    "                self.mkblock()\n",
    "                self.score += self.reward\n",
    "                self.reward = 10\n",
    "            else:\n",
    "                self.reward = -100\n",
    "        else:\n",
    "            # 움직일 수 없는 경우는 게임종료\n",
    "            self.gameOver()\n",
    "\n",
    "    def canMove(self):\n",
    "        # 움직임이 가능한지 확인 움직임이 가능하면 ture\n",
    "        # 반환 불가능하면 게임 종료\n",
    "        for i in range(self.size):\n",
    "            for j in range(self.size - 1):\n",
    "                if self.bord[i][j] == self.bord[i][j + 1]:\n",
    "                    return True\n",
    "        for j in range(self.size):\n",
    "            for i in range(self.size - 1):\n",
    "                if self.bord[i][j] == self.bord[i + 1][j]:\n",
    "                    return True\n",
    "        if not self.isfull():\n",
    "            return True\n",
    "        self.done = True\n",
    "        return False\n",
    "\n",
    "    def gameOver(self):\n",
    "        # done 게임 종료 선언\n",
    "        self.done = True\n",
    "        self.reward = self.score\n",
    "\n",
    "    def reset(self):\n",
    "        self.bord = np.zeros((self.size, self.size), dtype='int64')\n",
    "        self.score = 0\n",
    "        self.reward = 0\n",
    "        self.done = False\n",
    "\n",
    "    def start(self):\n",
    "        # 게임 시작\n",
    "        self.mkblock()\n",
    "        self.mkblock()\n",
    "        return self.bord\n",
    "\n",
    "    def getBord(self):\n",
    "        return self.bord\n",
    "\n",
    "    def getScore(self):\n",
    "        return self.score\n",
    "\n",
    "    def getReward(self):\n",
    "        return self.reward\n",
    "\n",
    "    def isDone(self):\n",
    "        return self.done\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import deque\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "EPISODES = 2000\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size, pretrain=False):\n",
    "        self.load_model = pretrain\n",
    "        self.render = True\n",
    "        self.discount_factor = 0.99\n",
    "        self.learning_rate = 0.01\n",
    "\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay = 0.99999\n",
    "        self.epsilon_min = 0.2\n",
    "\n",
    "        self.batch_size = 256\n",
    "        self.train_start = 1000\n",
    "\n",
    "        self.queueLenMax = 3000\n",
    "        self.memory = deque(maxlen=self.queueLenMax)\n",
    "\n",
    "        self.perfWindowSize = 10\n",
    "        self.penalty = -100\n",
    "\n",
    "        self.nNodesLayer1 = 64\n",
    "        self.nNodesLayer2 = 32\n",
    "\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        self.model = self.build_model()\n",
    "        self.target_model = self.build_model()\n",
    "\n",
    "        self.update_target_model()  # set same weights at the first time\n",
    "\n",
    "        if self.load_model:\n",
    "            self.model.load_weights(\"./save_model/2048.h5\")\n",
    "            self.epsilon_decay = 1\n",
    "            self.epsilon = 0\n",
    "            self.epsilon_min = 0\n",
    "\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(self.nNodesLayer1, input_dim=self.state_size, activation='relu', init='he_uniform'))  # , kernel_initializer ='he_uniform'))\n",
    "        model.add(Dense(self.nNodesLayer2, activation='relu', init='he_uniform'))  # , kernel_initializer ='he_uniform'))\n",
    "        model.add(Dense(self.action_size, activation='linear', init='he_uniform'))  # , kernel_initializer ='he_uniform'))\n",
    "        model.summary()\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def get_action(self, state):\n",
    "        s = state.reshape(1, -1)\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            q_value = self.model.predict(s)\n",
    "            print(q_value)\n",
    "            return np.argmax(q_value[0])\n",
    "\n",
    "    def append_sample(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def train_model(self):\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "        else:\n",
    "            self.epsilon = self.epsilon_min\n",
    "\n",
    "        mini_batch = random.sample(self.memory, self.batch_size)\n",
    "\n",
    "        states = np.zeros((self.batch_size, self.state_size))\n",
    "        next_states = np.zeros((self.batch_size, self.state_size))\n",
    "        actions, rewards, dones = [], [], []\n",
    "        for i in range(self.batch_size):\n",
    "            states[i] = mini_batch[i][0].reshape(-1)\n",
    "            actions.append(mini_batch[i][1])\n",
    "            rewards.append(mini_batch[i][2])\n",
    "            next_states[i] = mini_batch[i][3].reshape(-1)\n",
    "            dones.append(mini_batch[i][4])\n",
    "\n",
    "        target = self.model.predict(states)\n",
    "        target_val = self.target_model.predict(next_states)\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "\n",
    "            if dones[i]:\n",
    "                target[i][actions[i]] = rewards[i]\n",
    "            else:\n",
    "                target[i][actions[i]] = rewards[i] + self.discount_factor * (np.amax(target_val[i]))\n",
    "\n",
    "        # and do the model fit!\n",
    "        self.model.fit(states, target, batch_size=self.batch_size, epochs=1, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "dense_1 (Dense)                  (None, 64)            1088        dense_input_1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 32)            2080        dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 4)             132         dense_2[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 3,300\n",
      "Trainable params: 3,300\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "dense_4 (Dense)                  (None, 64)            1088        dense_input_2[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_5 (Dense)                  (None, 32)            2080        dense_4[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_6 (Dense)                  (None, 4)             132         dense_5[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 3,300\n",
      "Trainable params: 3,300\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "Episode: 0  score: 636  memory length 152  epsilon: 1.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-7ca8a9a1008d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m#             if i_episode > 990:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m             \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-d6fcb77e523e>\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmake_move\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmove\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Python36\\lib\\tkinter\\__init__.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1175\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1176\u001b[0m         \u001b[1;34m\"\"\"Enter event loop until all pending events have been processed by Tcl.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1177\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'update'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1178\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mupdate_idletasks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m         \"\"\"Enter event loop until all idle callbacks have been called. This\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    env = Env()\n",
    "    state_size = env.get_state().reshape(1, -1).shape[1]\n",
    "    action_size = len(env.action_space)\n",
    "    agent = DQNAgent(state_size, action_size, pretrain=False)\n",
    "\n",
    "    maxScore = -np.inf\n",
    "\n",
    "\n",
    "    s = []\n",
    "    for i_episode in range(1000):\n",
    "        state = env.reset()\n",
    "        score = 0\n",
    "        while True:\n",
    "#             if i_episode > 990:\n",
    "            env.render()\n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            agent.append_sample(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            if (agent.load_model is False) and (len(agent.memory) >= agent.train_start):\n",
    "                agent.train_model()\n",
    "\n",
    "            if done:\n",
    "                s.append(reward)\n",
    "                print('Episode:', i_episode, ' score:', reward, ' memory length', len(agent.memory), ' epsilon:', agent.epsilon)\n",
    "                if agent.load_model is False:\n",
    "                    agent.update_target_model()\n",
    "                    if reward > maxScore:\n",
    "                        maxScore = reward\n",
    "#                         agent.model.save_weights('save_model/2048.h5')\n",
    "                break\n",
    "    plt.plot(s)\n",
    "    plt.title('Score per episode, Learning_rate = {}, epsilon_min={}'.format(agent.learning_rate, agent.epsilon_min))\n",
    "    plt.savefig('save_plt/Learning_rate={},epsilon_min={},Discount_factor={},episod={}.png'.format(agent.learning_rate, agent.epsilon_min, agent.discount_factor, i_episode+1), dpi=300, facecolor='w')\n",
    "    plt.show()\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pre-trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_24 (Dense)             (None, 64)                1088      \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 4)                 132       \n",
      "=================================================================\n",
      "Total params: 3,300\n",
      "Trainable params: 3,300\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_27 (Dense)             (None, 64)                1088      \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 4)                 132       \n",
      "=================================================================\n",
      "Total params: 3,300\n",
      "Trainable params: 3,300\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "TclError",
     "evalue": "invalid command name \".!frame.!frame.!label\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTclError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-a0d6614250ae>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m             \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m             \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend_sample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-d6fcb77e523e>\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     94\u001b[0m                 \u001b[0mnew_number\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mnew_number\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrid_cells\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBACKGROUND_COLOR_CELL_EMPTY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m                     self.grid_cells[i][j].configure(text=str(new_number), bg=self.BACKGROUND_COLOR_DICT[new_number],\n",
      "\u001b[1;32m~\\anaconda3\\lib\\tkinter\\__init__.py\u001b[0m in \u001b[0;36mconfigure\u001b[1;34m(self, cnf, **kw)\u001b[0m\n\u001b[0;32m   1635\u001b[0m         \u001b[0mthe\u001b[0m \u001b[0mallowed\u001b[0m \u001b[0mkeyword\u001b[0m \u001b[0marguments\u001b[0m \u001b[0mcall\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0mkeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1636\u001b[0m         \"\"\"\n\u001b[1;32m-> 1637\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_configure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'configure'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcnf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1638\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1639\u001b[0m     \u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfigure\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\tkinter\\__init__.py\u001b[0m in \u001b[0;36m_configure\u001b[1;34m(self, cmd, cnf, kw)\u001b[0m\n\u001b[0;32m   1625\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcnf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1626\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getconfigure1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_flatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_w\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'-'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mcnf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1627\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_flatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_w\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_options\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcnf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1628\u001b[0m     \u001b[1;31m# These used to be defined in Widget:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1629\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTclError\u001b[0m: invalid command name \".!frame.!frame.!label\""
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    env = Env()\n",
    "    state_size = env.get_state().reshape(1, -1).shape[1]\n",
    "    action_size = len(env.action_space)\n",
    "    agent = DQNAgent(state_size, action_size, pretrain=True)\n",
    "\n",
    "    maxScore = -np.inf\n",
    "\n",
    "\n",
    "    s = []\n",
    "    for i_episode in range(20):\n",
    "        state = env.reset()\n",
    "        score = 0\n",
    "        while True:\n",
    "            env.render()\n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            agent.append_sample(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            if (agent.load_model is False) and (len(agent.memory) >= agent.train_start):\n",
    "                agent.train_model()\n",
    "\n",
    "            if done:\n",
    "                s.append(reward)\n",
    "                print('Episode:', i_episode, ' score:', reward, ' memory length', len(agent.memory), ' epsilon:', agent.epsilon)\n",
    "                if agent.load_model is False:\n",
    "                    agent.update_target_model()\n",
    "                    if reward > maxScore:\n",
    "                        maxScore = reward\n",
    "                        agent.model.save_weights('save_model/2048.h5')\n",
    "                break\n",
    "    plt.plot(s)\n",
    "    plt.title('Score per episode, Learning_rate = {}, epsilon_min={}'.format(agent.learning_rate, agent.epsilon_min))\n",
    "    if agent.load_model is False:\n",
    "        plt.savefig('save_plt/Learning_rate={},epsilon_min={},Discount_factor={},episod={}.png'.format(agent.learning_rate, agent.epsilon_min, agent.discount_factor, i_episode+1), dpi=300, facecolor='w')\n",
    "    plt.show()\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
