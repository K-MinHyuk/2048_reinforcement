{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "inside-single",
   "metadata": {},
   "source": [
    "# 2048\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "focused-steering",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random as rand\n",
    "import time\n",
    "from tkinter import *\n",
    "import tkinter as tk\n",
    "\n",
    "FONT = (\"Verdana\", 40, \"bold\")\n",
    "\n",
    "\n",
    "class Env(tk.Tk):\n",
    "    def __init__(self):\n",
    "        super(Env, self).__init__()\n",
    "        self.title('2048')\n",
    "        self.geometry('{0}x{1}'.format(700, 800))\n",
    "        self.SIZE = 500\n",
    "        self.GRID_LEN = 4\n",
    "        self.GRID_PADDING = 10\n",
    "        self.BACKGROUND_COLOR_GAME = \"#92877d\"\n",
    "        self.BACKGROUND_COLOR_CELL_EMPTY = \"#9e948a\"\n",
    "        self.BACKGROUND_COLOR_DICT = {2: \"#eee4da\", 4: \"#ede0c8\", 8: \"#f2b179\", 16: \"#f59563\",\n",
    "                                      32: \"#f67c5f\", 64: \"#f65e3b\", 128: \"#edcf72\", 256: \"#edcc61\",\n",
    "                                      512: \"#edc850\", 1024: \"#edc53f\", 2048: \"#edc22e\"}\n",
    "\n",
    "        self.CELL_COLOR_DICT = {2: \"#776e65\", 4: \"#776e65\", 8: \"#f9f6f2\", 16: \"#f9f6f2\",\n",
    "                                32: \"#f9f6f2\", 64: \"#f9f6f2\", 128: \"#f9f6f2\", 256: \"#f9f6f2\",\n",
    "                                512: \"#f9f6f2\", 1024: \"#f9f6f2\", 2048: \"#f9f6f2\"}\n",
    "        self.grid_cells = []\n",
    "        self.init_grid()\n",
    "        self.init_matrix()\n",
    "        self.action_space = ['u', 'd', 'l', 'r']\n",
    "        self.action_size = len(self.action_space)\n",
    "        self.counter = 0\n",
    "        self.wait_visibility()\n",
    "\n",
    "    def init_grid(self):\n",
    "        self.background = tk.Frame(self, bg=self.BACKGROUND_COLOR_GAME, width=self.SIZE, height=self.SIZE)\n",
    "        self.background.grid()\n",
    "        for i in range(self.GRID_LEN):\n",
    "            self.grid_row = []\n",
    "            for j in range(self.GRID_LEN):\n",
    "                self.cell = tk.Frame(self.background, bg=self.BACKGROUND_COLOR_CELL_EMPTY,\n",
    "                                     width=self.SIZE / self.GRID_LEN, height=self.SIZE / self.GRID_LEN)\n",
    "                self.cell.grid(row=i, column=j, padx=self.GRID_PADDING, pady=self.GRID_PADDING)\n",
    "                # font = Font(size=FONT_SIZE, family=FONT_FAMILY, weight=FONT_WEIGHT)\n",
    "                t = Label(master=self.cell, text=\"\", bg=self.BACKGROUND_COLOR_CELL_EMPTY, justify=CENTER, font=FONT,\n",
    "                          width=4, height=2)\n",
    "                t.grid()\n",
    "                self.grid_row.append(t)\n",
    "\n",
    "            self.grid_cells.append(self.grid_row)\n",
    "        self.grid_row = []\n",
    "        self.cell = tk.Frame(self.background, bg=self.BACKGROUND_COLOR_CELL_EMPTY, width=self.SIZE,\n",
    "                             height=self.SIZE / self.GRID_LEN)\n",
    "        self.cell.grid(row=4, columnspan=4, padx=self.GRID_PADDING, pady=self.GRID_PADDING, sticky=W + E + N + S)\n",
    "        t = Label(master=self.cell, text=\"\", bg=self.BACKGROUND_COLOR_CELL_EMPTY, justify=CENTER, font=FONT, width=4,\n",
    "                  height=2)\n",
    "        t.grid(columnspan=4, ipadx=250)\n",
    "        self.grid_row.append(t)\n",
    "        self.grid_cells.append(self.grid_row)\n",
    "\n",
    "    def init_matrix(self):\n",
    "        self.game = Game(self.GRID_LEN)\n",
    "        self.state = self.game.start()\n",
    "\n",
    "    def render(self):\n",
    "        time.sleep(0.01)\n",
    "        self.update()\n",
    "\n",
    "    def make_move(self, move=None):\n",
    "        #         self.title.configure(f'2048 : {self.game.score}')\n",
    "        #         print(self.game.score)\n",
    "\n",
    "        #         output = 0 # 이 부분이 인공 신경망에서 전달해준 값을 바탕으로 선택된 action에 전달될 움직임 방향 -> 나중에 적용할 때 매개변수로 받고 환경에서 부르면 될듯\n",
    "        #         move = rand.choice([0,1,2,3]) # np.argmax(output[0]) 이런식으로 신경망이 뱉어준 움직임을 선택하거나 따로 인덱스를 주거나 해서 선택하면 될듯\n",
    "        self.game.move(move)\n",
    "        self.state = self.game.getBord()\n",
    "        return self.state\n",
    "\n",
    "    def reset(self):\n",
    "        self.game.reset()\n",
    "        self.init_matrix()\n",
    "        return self.game.getBord().reshape(-1, 1)\n",
    "\n",
    "    def get_state(self):\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        self.counter += 1\n",
    "        next_state = self.make_move(action).reshape(-1, 1)\n",
    "        reward = self.game.getReward()\n",
    "        done = self.game.isDone()\n",
    "        for i in range(self.GRID_LEN):\n",
    "            for j in range(self.GRID_LEN):\n",
    "                new_number = self.state[i][j]\n",
    "                if new_number == 0:\n",
    "                    self.grid_cells[i][j].configure(text=\"\", bg=self.BACKGROUND_COLOR_CELL_EMPTY)\n",
    "                else:\n",
    "                    self.grid_cells[i][j].configure(text=str(new_number), bg=self.BACKGROUND_COLOR_DICT[new_number],\n",
    "                                                    fg=self.CELL_COLOR_DICT[new_number])\n",
    "        self.grid_cells[4][0].configure(text=\"Game Score : {}\".format(self.game.getScore()),\n",
    "                                        bg=self.BACKGROUND_COLOR_CELL_EMPTY, justify=CENTER)\n",
    "        if done:\n",
    "            self.grid_cells[1][1].configure(text=\"You\", bg=self.BACKGROUND_COLOR_CELL_EMPTY)\n",
    "            self.grid_cells[1][2].configure(text=\"Lose!\", bg=self.BACKGROUND_COLOR_CELL_EMPTY)\n",
    "            reward = self.game.getScore()\n",
    "        return next_state, reward, done\n",
    "    \n",
    "    def close(self):\n",
    "        self.destroy()\n",
    "\n",
    "\n",
    "\n",
    "class Game:\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "        self.bord = np.zeros((self.size, self.size), dtype='int64')\n",
    "        self.score = 0\n",
    "        self.reward = 0\n",
    "        self.done = False\n",
    "\n",
    "    def findEmpty(self):\n",
    "        # 빈칸을 확인해서 좌표 return / 없으면 None return\n",
    "        z_list = np.array([[r, c] for r, c in zip(np.where(self.bord == 0)[0], np.where(self.bord == 0)[1])])\n",
    "        if z_list.size == 0:\n",
    "            return None\n",
    "        location = rand.choice(z_list)\n",
    "        return location\n",
    "\n",
    "    def isfull(self):\n",
    "        # bord가 숫자로 가득 차있는경우 True/ 빈칸이 있는경우 False 반환\n",
    "        if 0 in self.bord.reshape(-1):\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "    def mkblock(self):\n",
    "        # 빈칸에 확률에 맞춰서 2또는 4 블럭을 생성\n",
    "        location = self.findEmpty()\n",
    "        self.bord[location[0]][location[1]] = 2 if rand.random() < 0.9 else 4\n",
    "\n",
    "    def merge(self, lstin):\n",
    "        # 합치는 연산\n",
    "        lst = lstin\n",
    "        if lst.size != len(set(lst)):\n",
    "            for t in range(lst.size - 1):\n",
    "                if lst[t] == lst[t + 1]:\n",
    "                    self.reward += lst[t]\n",
    "                    lst[t] *= 2\n",
    "                    lst[t + 1] = 0\n",
    "            lst = lst[np.where(lst != 0)]\n",
    "        return lst\n",
    "\n",
    "    def move(self, index):\n",
    "        self.reward = 0  # 움직이기 전에 보상을 초기화\n",
    "        ismove = False  # 움직임 후 변화가 없는 경우에는 안 움직인 것으로 판단하기위해 설정\n",
    "        # index에 따라 움직임\n",
    "        # 움직임이 가능한지 확인하고\n",
    "        #         print( self.canMove())\n",
    "        if self.canMove():\n",
    "            # 가능하다면 움직임 (0 - 상) (1 - 하) (2 - 좌) (3 - 우)\n",
    "            if index == 0:\n",
    "                for c in range(self.size):\n",
    "                    col = self.bord[:, c][np.where(self.bord[:, c] != 0)]\n",
    "                    col = self.merge(col)\n",
    "                    pd = 4 - col.size\n",
    "                    if np.any(self.bord[:, c] != np.pad(col, (0, pd))):\n",
    "                        self.bord[:, c] = np.pad(col, (0, pd))\n",
    "                        ismove = True\n",
    "\n",
    "            elif index == 1:\n",
    "                for c in range(self.size):\n",
    "                    col = self.bord[:, c][np.where(self.bord[:, c] != 0)]\n",
    "                    col = self.merge(col[::-1])[::-1]\n",
    "                    pd = 4 - col.size\n",
    "                    if np.any(self.bord[:, c] != np.pad(col, (pd, 0))):\n",
    "                        self.bord[:, c] = np.pad(col, (pd, 0))\n",
    "                        ismove = True\n",
    "\n",
    "            elif index == 2:\n",
    "                for r in range(self.size):\n",
    "                    row = self.bord[r, :][np.where(self.bord[r, :] != 0)]\n",
    "                    row = self.merge(row)\n",
    "                    pd = 4 - row.size\n",
    "                    if np.any(self.bord[r, :] != np.pad(row, (0, pd))):\n",
    "                        self.bord[r, :] = np.pad(row, (0, pd))\n",
    "                        ismove = True\n",
    "\n",
    "            elif index == 3:\n",
    "                for r in range(self.size):\n",
    "                    row = self.bord[r, :][np.where(self.bord[r, :] != 0)]\n",
    "                    row = self.merge(row[::-1])[::-1]\n",
    "                    pd = 4 - row.size\n",
    "                    if np.any(self.bord[r, :] != np.pad(row, (pd, 0))):\n",
    "                        self.bord[r, :] = np.pad(row, (pd, 0))\n",
    "                        ismove = True\n",
    "\n",
    "            if ismove:  # 움직임이 있었던 경우에만 블럭 생성\n",
    "                self.mkblock()\n",
    "                self.score += self.reward\n",
    "                self.reward = 10\n",
    "            else:\n",
    "                self.reward = -100\n",
    "        else:\n",
    "            # 움직일 수 없는 경우는 게임종료\n",
    "            self.gameOver()\n",
    "\n",
    "    def canMove(self):\n",
    "        # 움직임이 가능한지 확인 움직임이 가능하면 ture\n",
    "        # 반환 불가능하면 게임 종료\n",
    "        for i in range(self.size):\n",
    "            for j in range(self.size - 1):\n",
    "                if self.bord[i][j] == self.bord[i][j + 1]:\n",
    "                    return True\n",
    "        for j in range(self.size):\n",
    "            for i in range(self.size - 1):\n",
    "                if self.bord[i][j] == self.bord[i + 1][j]:\n",
    "                    return True\n",
    "        if not self.isfull():\n",
    "            return True\n",
    "        self.done = True\n",
    "        return False\n",
    "\n",
    "    def gameOver(self):\n",
    "        # done 게임 종료 선언\n",
    "        self.done = True\n",
    "        self.reward = self.score\n",
    "\n",
    "    def reset(self):\n",
    "        self.bord = np.zeros((self.size, self.size), dtype='int64')\n",
    "        self.score = 0\n",
    "        self.reward = 0\n",
    "        self.done = False\n",
    "\n",
    "    def start(self):\n",
    "        # 게임 시작\n",
    "        self.mkblock()\n",
    "        self.mkblock()\n",
    "        return self.bord\n",
    "\n",
    "    def getBord(self):\n",
    "        return self.bord\n",
    "\n",
    "    def getScore(self):\n",
    "        return self.score\n",
    "\n",
    "    def getReward(self):\n",
    "        return self.reward\n",
    "\n",
    "    def isDone(self):\n",
    "        return self.done\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "lucky-uzbekistan",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import deque\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "EPISODES = 2000\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size, pretrain=False):\n",
    "        self.load_model = pretrain\n",
    "        self.render = True\n",
    "        self.discount_factor = 0.99\n",
    "        self.learning_rate = 0.01\n",
    "\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay = 0.99999\n",
    "        self.epsilon_min = 0.2\n",
    "\n",
    "        self.batch_size = 256\n",
    "        self.train_start = 1000\n",
    "\n",
    "        self.queueLenMax = 3000\n",
    "        self.memory = deque(maxlen=self.queueLenMax)\n",
    "\n",
    "        self.perfWindowSize = 10\n",
    "        self.penalty = -100\n",
    "\n",
    "        self.nNodesLayer1 = 64\n",
    "        self.nNodesLayer2 = 32\n",
    "\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        self.model = self.build_model()\n",
    "        self.target_model = self.build_model()\n",
    "\n",
    "        self.update_target_model()  # set same weights at the first time\n",
    "\n",
    "        if self.load_model:\n",
    "            self.model.load_weights(\"./save_model/2048.h5\")\n",
    "            self.epsilon_decay = 1\n",
    "            self.epsilon = 0\n",
    "            self.epsilon_min = 0\n",
    "\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(self.nNodesLayer1, input_dim=self.state_size, activation='relu', kernel_initializer='he_uniform'))  # , kernel_initializer ='he_uniform'))\n",
    "        model.add(Dense(self.nNodesLayer2, activation='relu', kernel_initializer='he_uniform'))  # , kernel_initializer ='he_uniform'))\n",
    "        model.add(Dense(self.action_size, activation='linear', kernel_initializer='he_uniform'))  # , kernel_initializer ='he_uniform'))\n",
    "        model.summary()\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def get_action(self, state):\n",
    "        s = state.reshape(1, -1)\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            q_value = self.model.predict(s)\n",
    "            print(q_value)\n",
    "            return np.argmax(q_value[0])\n",
    "\n",
    "    def append_sample(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def train_model(self):\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "        else:\n",
    "            self.epsilon = self.epsilon_min\n",
    "\n",
    "        mini_batch = random.sample(self.memory, self.batch_size)\n",
    "\n",
    "        states = np.zeros((self.batch_size, self.state_size))\n",
    "        next_states = np.zeros((self.batch_size, self.state_size))\n",
    "        actions, rewards, dones = [], [], []\n",
    "        for i in range(self.batch_size):\n",
    "            states[i] = mini_batch[i][0].reshape(-1)\n",
    "            actions.append(mini_batch[i][1])\n",
    "            rewards.append(mini_batch[i][2])\n",
    "            next_states[i] = mini_batch[i][3].reshape(-1)\n",
    "            dones.append(mini_batch[i][4])\n",
    "\n",
    "        target = self.model.predict(states)\n",
    "        target_val = self.target_model.predict(next_states)\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "\n",
    "            if dones[i]:\n",
    "                target[i][actions[i]] = rewards[i]\n",
    "            else:\n",
    "                target[i][actions[i]] = rewards[i] + self.discount_factor * (np.amax(target_val[i]))\n",
    "\n",
    "        # and do the model fit!\n",
    "        self.model.fit(states, target, batch_size=self.batch_size, epochs=1, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plastic-cover",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 64)                1088      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 132       \n",
      "=================================================================\n",
      "Total params: 3,300\n",
      "Trainable params: 3,300\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 64)                1088      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 4)                 132       \n",
      "=================================================================\n",
      "Total params: 3,300\n",
      "Trainable params: 3,300\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Episode: 0  score: 200  memory length 62  epsilon: 1.0\n",
      "Episode: 1  score: 510  memory length 195  epsilon: 1.0\n",
      "Episode: 2  score: 502  memory length 324  epsilon: 1.0\n",
      "Episode: 3  score: 638  memory length 489  epsilon: 1.0\n",
      "Episode: 4  score: 146  memory length 550  epsilon: 1.0\n",
      "Episode: 5  score: 562  memory length 682  epsilon: 1.0\n",
      "Episode: 6  score: 662  memory length 829  epsilon: 1.0\n",
      "Episode: 7  score: 332  memory length 931  epsilon: 1.0\n",
      "Episode: 8  score: 538  memory length 1069  epsilon: 0.9993002414452723\n",
      "Episode: 9  score: 330  memory length 1176  epsilon: 0.9982315566918025\n",
      "[[28.136873 29.843752 28.030024 21.13516 ]]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    env = Env()\n",
    "    state_size = env.get_state().reshape(1, -1).shape[1]\n",
    "    action_size = len(env.action_space)\n",
    "    agent = DQNAgent(state_size, action_size, pretrain=False)\n",
    "\n",
    "    maxScore = -np.inf\n",
    "\n",
    "\n",
    "    s = []\n",
    "    for i_episode in range(1000):\n",
    "        state = env.reset()\n",
    "        score = 0\n",
    "        while True:\n",
    "#             if i_episode > 990:\n",
    "            env.render()\n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            agent.append_sample(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            if (agent.load_model is False) and (len(agent.memory) >= agent.train_start):\n",
    "                agent.train_model()\n",
    "\n",
    "            if done:\n",
    "                s.append(reward)\n",
    "                print('Episode:', i_episode, ' score:', reward, ' memory length', len(agent.memory), ' epsilon:', agent.epsilon)\n",
    "                if agent.load_model is False:\n",
    "                    agent.update_target_model()\n",
    "                    if reward > maxScore:\n",
    "                        maxScore = reward\n",
    "#                         agent.model.save_weights('save_model/2048.h5')\n",
    "                break\n",
    "    plt.plot(s)\n",
    "    plt.title('Score per episode, Learning_rate = {}, epsilon_min={}'.format(agent.learning_rate, agent.epsilon_min))\n",
    "    plt.savefig('save_plt/Learning_rate={},epsilon_min={},Discount_factor={},episod={}.png'.format(agent.learning_rate, agent.epsilon_min, agent.discount_factor, i_episode+1), dpi=300, facecolor='w')\n",
    "    plt.show()\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increased-album",
   "metadata": {},
   "source": [
    "# pre-trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cultural-steal",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_24 (Dense)             (None, 64)                1088      \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 4)                 132       \n",
      "=================================================================\n",
      "Total params: 3,300\n",
      "Trainable params: 3,300\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_27 (Dense)             (None, 64)                1088      \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 4)                 132       \n",
      "=================================================================\n",
      "Total params: 3,300\n",
      "Trainable params: 3,300\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "TclError",
     "evalue": "invalid command name \".!frame.!frame.!label\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTclError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-a0d6614250ae>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m             \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m             \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend_sample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-d6fcb77e523e>\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     94\u001b[0m                 \u001b[0mnew_number\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mnew_number\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrid_cells\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBACKGROUND_COLOR_CELL_EMPTY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m                     self.grid_cells[i][j].configure(text=str(new_number), bg=self.BACKGROUND_COLOR_DICT[new_number],\n",
      "\u001b[1;32m~\\anaconda3\\lib\\tkinter\\__init__.py\u001b[0m in \u001b[0;36mconfigure\u001b[1;34m(self, cnf, **kw)\u001b[0m\n\u001b[0;32m   1635\u001b[0m         \u001b[0mthe\u001b[0m \u001b[0mallowed\u001b[0m \u001b[0mkeyword\u001b[0m \u001b[0marguments\u001b[0m \u001b[0mcall\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0mkeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1636\u001b[0m         \"\"\"\n\u001b[1;32m-> 1637\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_configure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'configure'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcnf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1638\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1639\u001b[0m     \u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfigure\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\tkinter\\__init__.py\u001b[0m in \u001b[0;36m_configure\u001b[1;34m(self, cmd, cnf, kw)\u001b[0m\n\u001b[0;32m   1625\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcnf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1626\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getconfigure1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_flatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_w\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'-'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mcnf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1627\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_flatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_w\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_options\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcnf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1628\u001b[0m     \u001b[1;31m# These used to be defined in Widget:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1629\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTclError\u001b[0m: invalid command name \".!frame.!frame.!label\""
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    env = Env()\n",
    "    state_size = env.get_state().reshape(1, -1).shape[1]\n",
    "    action_size = len(env.action_space)\n",
    "    agent = DQNAgent(state_size, action_size, pretrain=True)\n",
    "\n",
    "    maxScore = -np.inf\n",
    "\n",
    "\n",
    "    s = []\n",
    "    for i_episode in range(20):\n",
    "        state = env.reset()\n",
    "        score = 0\n",
    "        while True:\n",
    "            env.render()\n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            agent.append_sample(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            if (agent.load_model is False) and (len(agent.memory) >= agent.train_start):\n",
    "                agent.train_model()\n",
    "\n",
    "            if done:\n",
    "                s.append(reward)\n",
    "                print('Episode:', i_episode, ' score:', reward, ' memory length', len(agent.memory), ' epsilon:', agent.epsilon)\n",
    "                if agent.load_model is False:\n",
    "                    agent.update_target_model()\n",
    "                    if reward > maxScore:\n",
    "                        maxScore = reward\n",
    "                        agent.model.save_weights('save_model/2048.h5')\n",
    "                break\n",
    "    plt.plot(s)\n",
    "    plt.title('Score per episode, Learning_rate = {}, epsilon_min={}'.format(agent.learning_rate, agent.epsilon_min))\n",
    "    if agent.load_model is False:\n",
    "        plt.savefig('save_plt/Learning_rate={},epsilon_min={},Discount_factor={},episod={}.png'.format(agent.learning_rate, agent.epsilon_min, agent.discount_factor, i_episode+1), dpi=300, facecolor='w')\n",
    "    plt.show()\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dimensional-berkeley",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
